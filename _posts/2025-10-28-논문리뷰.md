---
title: "[논문리뷰] "
date: 2025-10-28 00:00:00 +0900
categories: ["Paper Review", "Image Generation"]
tags: [dit, diffusion-models]
math: true
mermaid: true
---

> Diffusion Transformer (DiT) 구조를 제안한 “[Scalable Diffusion Models with Transformers](https://arxiv.org/abs/2212.09748), ICCV 2023” 논문 리뷰입니다.
<details>
<summary>참고 자료</summary>
- [https://github.com/facebookresearch/DiT/tree/main](https://github.com/facebookresearch/DiT/tree/main)
- [https://kyujinpy.tistory.com/132](https://kyujinpy.tistory.com/132)
    - 제가 자주 참고하는 블로그인데, 항상 느끼는 것이지만 설명을 정성들여서 잘 해주십니다. 이번에도 많은 참고가 되었습니다 🙂

</details>


---


### Summary

- Latent diffusion model에서 denoising network로 사용되던 UNet을 Transformer 구조로 대체한 DiT 모델 제안
- Condition(timestep, label 등)을 Adaptive LayerNorm(AdaLN)으로 모델링
- 높은 생성 품질 달성, model capacity에 따른 scalability를 보임

### Introduction


DDPM, ADM ([Ablated Diffusion Model](https://arxiv.org/abs/2105.05233)), LDM ([Latent Diffusion Model](https://arxiv.org/abs/2112.10752))에 이르기까지, 그동안의 diffusion model에서 denoising을 담당하는 모델 아키텍쳐는 Convolutional UNet을 기반으로 설계되어 왔습니다. 아키텍쳐의 발전 과정에서 self-attention module이 들어가거나, adaptive normalization layer가 도입되기도 했으나, 여전히 근간은 ResNet block으로 이루어진 convolutional 네트워크였습니다.


![Latent diffusion model (a.k.a. Stable Diffusion) architecture from “High-Resolution Image Synthesis with Latent Diffusion Models”](/assets/img/posts/2025/2025-10-28-논문리뷰-1.png){: width="700" .shadow }


한편, Transformer의 출범(?) 이후 자연어처리 뿐만 아니라 다양한 비전 task에서도 Transformer 기반의 모델 아키텍쳐가 높은 성능을 보이면서 기존의 CNN을 대체하고 있습니다. 그러다보니 자연스럽게 “diffusion model이 반드시 UNet이어야만 할까?” 라는 궁금증이 들게 되었던 것 같습니다. 


이 논문에서는 UNet을 Transformer block으로 대체할 수 있음을 보이고, 높은 이미지 생성 성능을 달성했을 뿐만 아니라, 모델을 더 키울수록 더 높은 성능을 보이는 연산량에 따른 scalability가 있다는 것도 보였습니다. 그렇게 제안한 모델인 DiT는 어떻게 설계되어있는지, 핵심 요소는 무엇인지 살펴보도록 하겠습니다.


![image.png](/assets/img/posts/2025/2025-10-28-논문리뷰-2.png){: width="700" .shadow }


### Diffusion Transformer (DiT) Architecture


Latent diffusion model의 모델 구조를 요약해보면, pixel space ↔ latent space를 매핑해주는 VAE가 있고, VAE Encoder를 통해 만들어지는 latent에 noise를 가하고, UNet을 통해 반복적으로 denoising한 뒤 VAE decoder를 통해 pixel space로 보내지게 됩니다. Denoising을 위해 UNet은 latent에 가해진 noise를 예측하도록 학습하게 됩니다.


**Patchify**


DiT에서는 UNet 대신 N개의 DiT block이 denoising network의 역할을 하게 됩니다. Vision Transformer에서 이미지를 처리하기 위해 이미지를 sequence of patch로 나누어서 self-attention을 연산했던 것처럼, **DiT에서도 비슷하게 noised latent를 여러 patch로 나누어서 처리하게 됩니다.** 이 과정을 논문에서는 “Patchify”라고 이야기 합니다.


![image.png](/assets/img/posts/2025/2025-10-28-논문리뷰-3.png){: width="700" .shadow }


이후엔 N개의 DiT block을 거치면서 forward가 되는데, DiT 블럭은 Multi-head self-attention과 FFN으로 이루어져있는 흔히 볼 수 있는 Transformer block과 유사합니다. 이 때, 흥미로운 점은 condition을 처리하기 위한 설계 방법입니다. 


**Processing Condition in DiT**


다른 diffusion model들에서도 그랬듯이, denoising network에서는 noised latent와 더불어서 “현재 어떤 timestep인지”에 대한 정보를 condition으로 받아서 가해진 noise를 예측하게 됩니다. 뿐만 아니라, 특정 class에 대한 이미지를 생성하기 위해 클래스 정보를 condition으로 받는다거나, text-to-image 생성을 위해 prompt를 condition으로 받는 경우들이 흔하게 있기 때문에 이런 다양한 condition을 denoising할 때 어떤 식으로 반영할 것인지가 중요한 설계 요소입니다.


**AdaLN-Zero**


DiT에서는 여러 가지 방법을 통해 condition을 처리하려는 시도를 했었는데요, LDM에서와 비슷하게 cross-attention module을 통해 처리해보기도 하고, 단순하게 이미지 토큰에 condition embedding을 concat해서 처리해보기도 했습니다. 여러 가지 variant 중, 실험을 통해 가장 효과가 좋았던 방법은 Adaptive LayerNorm(adaLN) block을 도입한 방법이었습니다. 원래 LayerNorm에는 scale과 shift를 담당하는 learnable한 affine parameter가 있는데요, **adaLN에서는 affine parameter를 condition으로부터 MLP를 거쳐서 도출해내는 방식을 사용합니다.** 즉, condition → scale, shift 파라미터로 만들어서 denoising에 반영하는 방법인 것이죠. 또한, 학습의 안정성을 위해 adaLN DiT block을 최초에 identity function이 되도록 초기화하는 테크닉(i.e., adaLN-Zero)을 도입했습니다. 


![image.png](/assets/img/posts/2025/2025-10-28-논문리뷰-4.png){: width="700" .shadow }


실제 implementation을 살펴보면 매우 간단한 구조로 되어있습니다. MLP를 통해 condition embedding으로부터 affine parameter들을 출력하고, 이를 attention 또는 FFN 전,후에 도입해서 사용하게 됩니다.


![Implementation of DiT from https://github.com/facebookresearch/DiT/blob/main/models.py#L101](/assets/img/posts/2025/2025-10-28-논문리뷰-5.png){: width="700" .shadow }


**Transformer Decoder**


N개의 DiT block으로 forward되면 `패치 수 x hidden dimension` 의 형태를 띄게 될텐데, 이를 noise를 예측하는 데에 적합하게 decoding해주어야 합니다. **DiT에서는 단순하게 LayerNorm(앞서서 adaLN을 사용했다면 여기도 adaLN) 이후 linear layer를 통과시킨 후 reshape하는 방식으로 decoding하게 됩니다.**


### Training & Inference of DiT


DiT의 학습은 이전에 제안되어 왔던 표준적인 diffusion model 학습과 동일합니다. 다만, **가장 단순하고 간단한 형태는 noise만 예측하는** $L=||\epsilon_\theta(x_t)-\epsilon_t||^2_2$ **일테지만 여기서는** [**“Improved Denoising Diffusion Probabilistic Models”**](https://arxiv.org/abs/2102.09672) **을 따라 covariance도 예측하도록 학습합니다.**


noise를 예측하는 과정은 diffusion process에서 noise를 가할 때 사용된 Gaussian distribution의 mean을 모델을 통해 추정하는 것이라고 볼 수 있는데요, ADM에서는 noise를 가할 때 적용된 covariance matrix도 함께 추정하는 방식의 loss를 적용했었습니다. 이를 참고해서 DiT를 학습할 때에도 모델이 가해진 noise  distribution의 평균만 추정하는 것이 아니라 분산도 추정하도록 한 것이죠.


샘플링을 할 때에는 noise로부터 시작해서 학습한 DiT를 통해 반복적으로 denoising하는 표준적인 방법을 통해 이미지를 생성하게 됩니다. 다른 방법들과 마찬가지로, Classifier-free guidance를 사용합니다.


### Experiments


ImageNet에 대한 class-conditional 이미지 생성 성능을 비교하는 실험을 진행했고, 기존의 UNet 기반의 diffusion model(ADM, LDM)들보다 우수한 성능을 보였습니다.


![image.png](/assets/img/posts/2025/2025-10-28-논문리뷰-6.png){: width="700" .shadow }


또한, 여러 가지 실험을 통해 DiT가 scalable하다는 것을 보였습니다. patch size를 줄이거나, 모델의 hidden dimension을 키우는 등의 여러 variant에 대해 실험했고, 모델의 size가 커질수록 일관되게 성능이 향상되었습니다.


![image.png](/assets/img/posts/2025/2025-10-28-논문리뷰-7.png){: width="700" .shadow }


### Conclusion


이번엔 최근 대부분의 이미지 생성 모델에서 표준적으로 채택되고 있는 diffusion transformer 아키텍쳐를 제안한 논문을 리뷰해봤습니다. 아키텍쳐 관련 연구는 논문만 보는 것보다 코드를 같이 보는 편이 확실히 더 이해가 잘 되는 것 같습니다.

